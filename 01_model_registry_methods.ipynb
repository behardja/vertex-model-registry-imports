{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d680f0-02ba-4f57-832c-f635d59c2d8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Log Artfiacts and Import Models into Registries\n",
    "\n",
    "Objective: various methods to bring existing models into Vertex Model Registry using Pre-built Containers and Custom Serving Containers. Uses user-supplied files including the dockerfile.\n",
    "\n",
    "**Prereqs:**\n",
    "* Create Artifact Registry repository for Docker images: https://cloud.google.com/artifact-registry/docs/repositories/create-repos#create\n",
    "* Enable API's:\n",
    "    * Vertex AI\n",
    "    * Cloud Storage\n",
    "    * Artifact Registry\n",
    "\n",
    "**Resources:**\n",
    "* [Import models to Vertex AI](https://cloud.google.com/vertex-ai/docs/model-registry/import-model#aiplatform_upload_model_sample-python_vertex_ai_sdk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e458a-e217-4a03-b283-def8e6a0c3ff",
   "metadata": {},
   "source": [
    "- `PROJECT_ID`: Google Cloud project ID where Vertex AI resources are deployed\n",
    "- `REGION`: Google Cloud region to deploy Vertex AI resources.\n",
    "- `VERSION`: Version tag for the Docker image.\n",
    "- `REPO_NAME`: Name of the Artifact Registry repository for Docker images.\n",
    "- `JOB_IMAGE_ID`: Name of the Docker image.\n",
    "- `BUCKET_URI`: Google Cloud Storage bucket URI used for storing model artifacts and staging data.\n",
    "- `model_file`: Name of the model file.\n",
    "- `SERVICE_ACCOUNT`: Service account used to run Vertex AI jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db432a3-97f9-4a43-9223-109c423768ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image Parameters\n",
    "PROJECT_ID = \"sandbox-401718\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\" # @param {type:\"string\"}\n",
    "VERSION=\"latest\" \n",
    "REPO_NAME=\"housing-poc\" # @param {type:\"string\"}\n",
    "JOB_IMAGE_ID=\"housing-poc-image\" # @param {type:\"string\"}\n",
    "\n",
    "# Cloud Storage \n",
    "BUCKET_URI = \"gs://sandbox-401718-us-notebooks/gke-yaml\"  # @param {type:\"string\"}\n",
    "model_file = \"model.pkl\"\n",
    "\n",
    "# Vertex Custom Job parameters\n",
    "SERVICE_ACCOUNT=\"757654702990-compute@developer.gserviceaccount.com\" # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bf21e0-eb9b-4a81-ad95-e7823c0b9350",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Method 1: Import Model to Model Registry using Pre-built Containers\n",
    "\n",
    "The most simple method which leverages pre-built container images by Google Cloud. These images support several common ML frameworks, including **Tensorflow, sklearn, PyTorch, and XGBoost.** The user just needs to supply model artifacts that comply with framework specific requirements to import the model into Vertex AI.\n",
    "\n",
    "\n",
    "![method_1.png](./imgs/method_1.png)\n",
    "\n",
    "\n",
    "List of pre-built containers: https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#expandable-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e5bd7-4997-4df6-8dc8-5e1d7a3cc52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65faf903-b497-45d3-9b21-e0b41e147958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model to cloud storage to be imported into modle registry\n",
    "\n",
    "! gsutil cp ./{model_file} {BUCKET_URI}/test/{model_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303b3f4-267b-4608-aa57-f78490dbc50b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import to model registry\n",
    "DEPLOY_IMAGE_URI = \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-5:latest\"\n",
    "display_model = \"housing-poc-model\"\n",
    "\n",
    "vertex_model = aiplatform.Model.upload(\n",
    "    display_name=display_model,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE_URI,\n",
    "    artifact_uri=f\"{BUCKET_URI}/test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab8dd7c-a28d-4865-9844-deb46934b97a",
   "metadata": {},
   "source": [
    "## Method 2: Import to Model Registry using Custom Serving Containers\n",
    "\n",
    "This method requires users to be responsible for direct development of their own containers. This provides the most granular-level control, enabling users to provide the serving container rather than just the model artifact (see pre-built containers in Method 1). Users can customize the container to specific requirements such as model architectures, dependencies, and serving logic. [Custom Container Requirements](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements)\n",
    "\n",
    "For custom containers on Vertex AI, model artifacts can either be bundled in the image (Method A) or decoupled by storing them separately in Cloud Storage (Method B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b863ad95-c03c-4de5-87a6-e40112439e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "# from google.cloud.aiplatform import explain\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc1b75-ef79-48d0-b6dd-2a61b4b63c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build and push image to reigstry\n",
    "! docker build -f ./Dockerfile -t {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{JOB_IMAGE_ID}:{VERSION} .\n",
    "! gcloud auth configure-docker us-central1-docker.pkg.dev --quiet\n",
    "! docker push {REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{JOB_IMAGE_ID}:{VERSION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e87de2-95ae-4583-9ab3-5f858df6e5a2",
   "metadata": {},
   "source": [
    "### Method 2A: Containers with the model included\n",
    "\n",
    "If your custom serving container image contains all the model artifacts required to serve predictions, you can register a Model in the Model Registry by providing the `serving container image` parameters.\n",
    "\n",
    "![method_2a.png](./imgs/method_2a.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d889c-1fd1-488e-87c6-5503418d8370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vertex_model = aiplatform.Model.upload(\n",
    "        display_name=display_model,\n",
    "        serving_container_image_uri=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{JOB_IMAGE_ID}:{VERSION}\",\n",
    "        serving_container_predict_route = \"/predict\",\n",
    "        serving_container_health_route = \"/health\",\n",
    "        serving_container_ports=[8080]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d86c7-2df4-40e3-872f-e5a3af76cef8",
   "metadata": {},
   "source": [
    "### Method 2B: Containers without the model included\n",
    "\n",
    "If model artifacts are stored separately in Cloud Storage, the container downloads them at startup using the AIP_STORAGE_URI environment variable provided by Vertex AI. Users can then register the model in the Model Registry by specifying the `custom serving image` along with the `artifact_uri` parameters.\n",
    "\n",
    "\n",
    "Documenation for [Accessing Model Artifacts](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#artifacts)\n",
    "\n",
    "![method_2b.png](./imgs/method_2b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd9f4c-2d6d-4f3f-8826-55d932f75c01",
   "metadata": {},
   "source": [
    "Below is a possible example of a serving application that installs the model artifact into the serving container.\n",
    "\n",
    "`main.py`\n",
    "\n",
    "```{python} \n",
    "## Example serving application\n",
    "\n",
    "import os\n",
    "from typing import Annotated, get_type_hints\n",
    "\n",
    "import joblib\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pandera import DataFrameModel\n",
    "from pandera.typing import DataFrame as DataFrame\n",
    "from pydantic import WithJsonSchema\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "# # Load your model\n",
    "model = None\n",
    "InputType = Annotated[DataFrame[DataFrameModel], WithJsonSchema(DataFrameModel.to_json_schema())]\n",
    "OutputType = Annotated[DataFrame[DataFrameModel], WithJsonSchema(DataFrameModel.to_json_schema())]\n",
    "\n",
    "app = FastAPI()\n",
    "        \n",
    "AIP_STORAGE_URI = os.environ.get('AIP_STORAGE_URI')\n",
    "if AIP_STORAGE_URI:\n",
    "    command = f\"gcloud storage cp '{AIP_STORAGE_URI}/model.pkl' model.pkl\"\n",
    "    subprocess.run(command, shell=True, stdout=subprocess.PIPE)\n",
    "    with open('model.pkl', 'rb') as f:\n",
    "        model = joblib.load(f)\n",
    "        types = get_type_hints(model.predict)\n",
    "        r = types.pop('return')\n",
    "        i = list(types.items())[0][1]\n",
    "        InputType = Annotated[DataFrame[i], WithJsonSchema(i.to_json_schema())]\n",
    "        OutputType = Annotated[DataFrame[r], WithJsonSchema(r.to_json_schema())]\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health() -> dict[str, str]:\n",
    "    if model is None:\n",
    "        return {\"STATUS\": \"ERROR\", \"MESSAGE\": \"Model not loaded\"}\n",
    "    return {\"STATUS\": \"OK\"}\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "def predict(data: InputType) -> OutputType:\n",
    "    return model.predict(pd.DataFrame(data))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d46660-6b53-4cc1-8c9a-efcae37a5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_model = aiplatform.Model.upload(\n",
    "        display_name=display_model,\n",
    "        artifact_uri=f\"{BUCKET_URI}/test\",\n",
    "        serving_container_image_uri=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPO_NAME}/{JOB_IMAGE_ID}:{VERSION}\",\n",
    "        serving_container_predict_route = \"/predict\",\n",
    "        serving_container_health_route = \"/health\",\n",
    "        serving_container_ports=[8080]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60290d5c-00c3-4065-a9fa-8aa3055f4288",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Method 3: Custom Prediction Routines\n",
    "\n",
    "Allows users to use the Vertex SDK to build custom containers with pre/post processing code. The SDK generates the Dockerfile and build images for the user. This offers a simplified interface for users to build and import models using containers by reducing the complexity of direct container development and deployment.\n",
    "\n",
    "\n",
    "Sample Notebook: [SDK_Pytorch_Custom_Predict.ipynb](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/prediction/custom_prediction_routines/SDK_Pytorch_Custom_Predict.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec70b3-db82-48f9-8930-252ca459391e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
